{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17721226-d585-456d-8c7e-51f24bac13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "from src.slurm import init_signal_handler, init_distributed_mode\n",
    "from src.data.loader import check_data_params, load_data\n",
    "from src.utils import bool_flag, initialize_exp, set_sampling_probs, shuf_order\n",
    "from src.model import check_model_params #build_model\n",
    "from src.trainer import SingleTrainer, EncDecTrainer\n",
    "from src.evaluation.evaluator import SingleEvaluator, EncDecEvaluator\n",
    "from src.model.transformer import TransformerModel\n",
    "\n",
    "import apex\n",
    "import pickle\n",
    "from src.fp16 import network_to_half\n",
    "\n",
    "from src.data.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c25b3d8d-0f16-4bbc-8ed8-04135581b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(params, dico):\n",
    "    \"\"\"\n",
    "    Build model.\n",
    "    \"\"\"\n",
    "    if params.encoder_only:\n",
    "        # build\n",
    "        model = TransformerModel(params, dico, is_encoder=True, with_output=True)\n",
    "\n",
    "        # reload a pretrained model\n",
    "        if params.reload_model != '':\n",
    "            logger.info(\"Reloading model from %s ...\" % params.reload_model)\n",
    "            reloaded = torch.load(params.reload_model, map_location=lambda storage, loc: storage.cuda(params.local_rank))['model']\n",
    "            if all([k.startswith('module.') for k in reloaded.keys()]):\n",
    "                reloaded = {k[len('module.'):]: v for k, v in reloaded.items()}\n",
    "\n",
    "            # # HACK to reload models with less layers\n",
    "            # for i in range(12, 24):\n",
    "            #     for k in TRANSFORMER_LAYER_PARAMS:\n",
    "            #         k = k % i\n",
    "            #         if k in model.state_dict() and k not in reloaded:\n",
    "            #             logger.warning(\"Parameter %s not found. Ignoring ...\" % k)\n",
    "            #             reloaded[k] = model.state_dict()[k]\n",
    "\n",
    "            model.load_state_dict(reloaded)\n",
    "\n",
    "        logger.debug(\"Model: {}\".format(model))\n",
    "        logger.info(\"Number of parameters (model): %i\" % sum([p.numel() for p in model.parameters() if p.requires_grad]))\n",
    "\n",
    "        return model\n",
    "\n",
    "    else:\n",
    "        # build\n",
    "        encoder = TransformerModel(params, dico, is_encoder=True, with_output=True)  # TODO: only output when necessary - len(params.clm_steps + params.mlm_steps) > 0\n",
    "        decoder = TransformerModel(params, dico, is_encoder=False, with_output=True)\n",
    "\n",
    "        # reload a pretrained model\n",
    "        if params.reload_model != '':\n",
    "            enc_path, dec_path = params.reload_model.split(',')\n",
    "            assert not (enc_path == '' and dec_path == '')\n",
    "\n",
    "            # reload encoder\n",
    "            if enc_path != '':\n",
    "                #logger.info(\"Reloading encoder from %s ...\" % enc_path)\n",
    "                enc_reload = torch.load(enc_path, map_location=lambda storage, loc: storage.cuda(params.local_rank))\n",
    "                enc_reload = enc_reload['model' if 'model' in enc_reload else 'encoder']\n",
    "                if all([k.startswith('module.') for k in enc_reload.keys()]):\n",
    "                    enc_reload = {k[len('module.'):]: v for k, v in enc_reload.items()}\n",
    "                encoder.load_state_dict(enc_reload)\n",
    "\n",
    "            # reload decoder\n",
    "            if dec_path != '':\n",
    "                #logger.info(\"Reloading decoder from %s ...\" % dec_path)\n",
    "                dec_reload = torch.load(dec_path, map_location=lambda storage, loc: storage.cuda(params.local_rank))\n",
    "                dec_reload = dec_reload['model' if 'model' in dec_reload else 'decoder']\n",
    "                if all([k.startswith('module.') for k in dec_reload.keys()]):\n",
    "                    dec_reload = {k[len('module.'):]: v for k, v in dec_reload.items()}\n",
    "                decoder.load_state_dict(dec_reload, strict=False)\n",
    "\n",
    "        #logger.debug(\"Encoder: {}\".format(encoder))\n",
    "        #logger.debug(\"Decoder: {}\".format(decoder))\n",
    "        #logger.info(\"Number of parameters (encoder): %i\" % sum([p.numel() for p in encoder.parameters() if p.requires_grad]))\n",
    "        #logger.info(\"Number of parameters (decoder): %i\" % sum([p.numel() for p in decoder.parameters() if p.requires_grad]))\n",
    "\n",
    "        return encoder, decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "666b8987-550d-4a45-91b4-647ef3eec76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_text(batch, lengths, dico, params):\n",
    "    \"\"\"\n",
    "    Convert a batch of sentences to a list of text sentences.\n",
    "    \"\"\"\n",
    "    batch = batch.cpu().numpy()\n",
    "    lengths = lengths.cpu().numpy()\n",
    "\n",
    "    slen, bs = batch.shape\n",
    "    assert lengths.max() == slen and lengths.shape[0] == bs\n",
    "    assert (batch[0] == params.eos_index).sum() == bs\n",
    "    assert (batch == params.eos_index).sum() == 2 * bs\n",
    "    sentences = []\n",
    "\n",
    "    for j in range(bs):\n",
    "        words = []\n",
    "        for k in range(1, lengths[j]):\n",
    "            if batch[k, j] == params.eos_index:\n",
    "                break\n",
    "            words.append(dico[batch[k, j]])\n",
    "        sentences.append(\" \".join(words))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3607d8b4-11a1-4dd3-bfbb-596a59ab7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensors(s, dico):\n",
    "    unk_words = {}\n",
    "    SPECIAL_WORD = '<special%i>'\n",
    "    SPECIAL_WORDS = 10\n",
    "    indexed = []\n",
    "    count_unk = 0\n",
    "    for w in s:\n",
    "        word_id = dico.index(w, no_unk=False)\n",
    "\n",
    "        if 0 <= word_id < 4 + SPECIAL_WORDS and word_id != 3:\n",
    "            logger.warning('Found unexpected special word \"%s\" (%i)!!' % (w, word_id))\n",
    "            continue\n",
    "\n",
    "        assert word_id >= 0\n",
    "        indexed.append(word_id)\n",
    "        if word_id == dico.unk_index:\n",
    "            unk_words[w] = unk_words.get(w, 0) + 1\n",
    "            count_unk += 1\n",
    "    x1 = torch.tensor(indexed)\n",
    "    len1 = torch.tensor([len(indexed)])\n",
    "    return torch.reshape(x1, (x1.shape[0], -1)),  len1 #torch.reshape(len1, (len1.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "523987ef-39c0-46ca-bda2-f7a8c1978b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mt(params, encoder, decoder, lang1, lang2, eval_bleu, dico, str1, str2=None):\n",
    "        \"\"\"\n",
    "        Evaluate perplexity and next word prediction accuracy.\n",
    "        \"\"\"\n",
    "        #print(\"Evaluate_mt: data_set, lang1, lang2, eval_bleu: \", lang1, lang2, eval_bleu)\n",
    "        \n",
    "        #assert lang1 in params.langs\n",
    "        #assert lang2 in params.langs\n",
    "\n",
    "        x1, len1 = convert_to_tensors(str1, dico)\n",
    "        if str2!=None:\n",
    "            x2, len2 = convert_to_tensors(str2, dico)\n",
    "        \n",
    "        #Encoder.eval()\n",
    "        #Decoder.eval()\n",
    "        \n",
    "        #print(params.multi_gpu)\n",
    "        \n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        #encoder = Encoder.module if params.multi_gpu else Encoder\n",
    "        #decoder = Decoder.module if params.multi_gpu else Decoder\n",
    "\n",
    "        lang1_id = params.lang2id[lang1]\n",
    "        lang2_id = params.lang2id[lang2]\n",
    "\n",
    "        n_words = 0\n",
    "        xe_loss = 0\n",
    "        n_valid = 0\n",
    "\n",
    "        # store hypothesis to compute BLEU score\n",
    "        if eval_bleu:\n",
    "            hypothesis = []\n",
    "        \n",
    "        \n",
    "        #(x1, len1), (x2, len2) = batch\n",
    "            \n",
    "        langs1 = x1.clone().fill_(lang1_id)\n",
    "        if str2!=None:\n",
    "            langs2 = x2.clone().fill_(lang2_id)\n",
    "\n",
    "            # target words to predict\n",
    "        \"\"\"alen = torch.arange(len2.max(), dtype=torch.long, device=len2.device)\n",
    "        pred_mask = alen[:, None] < len2[None] - 1  # do not predict anything given the last target word\n",
    "        y = x2[1:].masked_select(pred_mask[:-1])\n",
    "        assert len(y) == (len2 - 1).sum().item()\n",
    "\n",
    "            # cuda\n",
    "        x1, len1, langs1, x2, len2, langs2, y = to_cuda(x1, len1, langs1, x2, len2, langs2, y)\"\"\"\n",
    "\n",
    "            # encode source sentence\n",
    "        enc1 = encoder('fwd', x=x1, lengths=len1, langs=langs1, causal=False)\n",
    "        enc1 = enc1.transpose(0, 1)\n",
    "\n",
    "            # decode target sentence\n",
    "        \"\"\"dec2 = decoder('fwd', x=x2, lengths=len2, langs=langs2, causal=True, src_enc=enc1, src_len=len1)\n",
    "\n",
    "            # loss\n",
    "        word_scores, loss = decoder('predict', tensor=dec2, pred_mask=pred_mask, y=y, get_scores=True)\n",
    "\n",
    "            # update stats\n",
    "        n_words += y.size(0)\n",
    "        xe_loss += loss.item() * len(y)\n",
    "        n_valid += (word_scores.max(1)[1] == y).sum().item()\"\"\"\n",
    "\n",
    "            # generate translation - translate / convert to text\n",
    "        if eval_bleu:\n",
    "            max_len = int(1.5 * len1.max().item() + 10)\n",
    "            if params.beam_size == 1:\n",
    "                generated, lengths = decoder.generate(enc1, len1, lang2_id, max_len=max_len)\n",
    "            else:\n",
    "                generated, lengths = decoder.generate_beam(\n",
    "                    enc1, len1, lang2_id, beam_size=params.beam_size,\n",
    "                    length_penalty=params.length_penalty,\n",
    "                    early_stopping=params.early_stopping,\n",
    "                    max_len=max_len\n",
    "                )\n",
    "            hypothesis.extend(convert_to_text(generated, lengths, dico, params))\n",
    "\n",
    "        return hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33d3972d-6730-4948-95e4-c5461b1353e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : fa9b8962c7b6\n",
      "['Personal Care , Men , Shampoo , Razor Head , More by Razor <unk>']\n"
     ]
    }
   ],
   "source": [
    "def main(params):\n",
    "\n",
    "    init_distributed_mode(params)\n",
    "\n",
    "    data = load_data(params)\n",
    "    \n",
    "    #dico = Dictionary.read_vocab(\"vocab\")\n",
    "    dico = data['dico']\n",
    "    \n",
    "    # build model\n",
    "    if params.encoder_only:\n",
    "        model = build_model(params, dico)\n",
    "    else:\n",
    "        encoder, decoder = build_model(params, data['dico'])\n",
    "\n",
    "    s1 = \"Personal Care , Unisex , Shower Gel , Razor Head , More by Razor Head\".split()\n",
    "    s2 = None\n",
    "    lang1 = 'pred'\n",
    "    lang2 = 'act'\n",
    "    sentense = evaluate_mt(params, encoder, decoder, lang1, lang2, True, dico, s1, s2)\n",
    "    print(sentense)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open('params.pik', 'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "    \n",
    "    MODEL_PATH = \"dumped/unsupMT_predact/zp4s968o2a/checkpoint.pth,dumped/unsupMT_predact/zp4s968o2a/checkpoint.pth\"\n",
    "    params.reload_model=MODEL_PATH\n",
    "    params.eval_only =  True\n",
    "\n",
    "    # check parameters\n",
    "    check_data_params(params)\n",
    "    check_model_params(params)\n",
    "\n",
    "    # run experiment\n",
    "    main(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc510ba-6f6e-4b1b-b497-2fc0423b6f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706b9822-6127-4704-82a5-04a31e20f9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
